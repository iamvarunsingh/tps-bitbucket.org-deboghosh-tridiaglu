PARALLEL (MPI) DIRECT SOLVER FOR A TRIDIAGONAL SYSTEM OF EQUATIONS

The following functions are available in src/TridiagLU (/include as the 
required header file):-

  tridiagLU  (a,b,c,x,n,ns,r,m) (Parallel tridiagonal solver)

    Solves the tridiagonal system in parallel by reordering the
    points such that the first point of each subdomain is placed
    at the end.

    The interior points are eliminated in parallel, resulting in
    a reduced system consisting of the first point of each sub-
    domain.

    This reduced system is solved either by the gather-and-
    solve (tridiagLUGS) or the recursive-doubling (tridiagLURD)
    algorithms.

  tridiagLUGS(a,b,c,x,n,ns,r,m) (Tridiagonal solver using the
                                 gather-and-solve strategy)

    Each of the "ns" systems is gathered on one processor, 
    solved in serial, and the solution scattered back. The
    parallelism lies in solving the "ns" different systems 
    by multiple processors (i.e., each processor solves 
    ~ns/nproc number of systems in serial).

** A test code is provided in /src/Test to check the tridiagonal
   solvers and test the walltimes.

------------------------------------------------------------------------

REFERENCES:-

tridiagLU() solves the parallel tridiagonal system by rearranging the 
equations such that the last points of each sub-domain are grouped at
the end. See doc/algorithm.pdf for a rough sketch.

------------------------------------------------------------------------

COMPILING:-

If obtained by cloning the GIT/SVN repository, run these commands:-
> autoreconf -i

This will generate the required files for:
> [CFLAGS="..."] ./configure [--with-mpidir=/path/to/mpi] [--prefix=/install/dir]
> make
> make install

If unpacked from tarball, then proceed with ./configure, make and
make install.

** To compile a serial version: use -Dserial as the compile flag

------------------------------------------------------------------------

TESTING:-

The tridiagonal solvers can be tested by running bin/TRIDIAGLU that is 
created by a successful compilation.

USING THESE FUNCTIONS:-

Copy the header file include/tridiagLU.h to the include folder of
the code calling these functions. Copy the files 
src/TridiagLU/tridiagLU.c
src/TridiagLU/tridiagLUGS.c
to the the code's src directory and include them while compiling.

*OR*

Place include/tridiagLU.h where the compiler can find it
(or use the compiler flag "-I/path/to/tridiagLU.h" while
compiling) and include $build_dir/src/TridiagLU/libTridiagLU.a 
while linking.

------------------------------------------------------------------------

FUNCTION ARGUMENTS:

    a   [0,ns-1]x[0,n-1] double**         subdiagonal entries
    b   [0,ns-1]x[0,n-1] double**         diagonal entries
    c   [0,ns-1]x[0,n-1] double**         superdiagonal entries
    x   [0,ns-1]x[0,n-1] double**         right-hand side (solution)
    n                    int              local size of the system
    ns                   int              number of systems to solve
    r                    TridiagLUTime*   structure containing the runtimes
                                            total_time
                                            stage1_time
                                            stage2_time
                                            stage3_time
                                            stage4_time
                         ** Note that these are process-specific. Calling 
                            function needs to do something to add/average 
                            them to get some global value.
                         ** Can be NULL if runtimes are not needed.
    m                    MPIContext*         MPI Context
                                             **See below
                                             **Can be set to NULL for
                                               a serial computation

  Return value (int) -> 0 (successful solve), -1 (singular system)

  Note:-
    x contains the final solution (right-hand side is replaced)
    a,b,c are not preserved
    On rank=0,        a[0] has to be zero.
    On rank=nproc-1,  c[n-1] has to be zero.

  For a serial tridiagonal solver, compile with the flag "-Dserial"
  or send NULL as the argument for the MPI Context.

------------------------------------------------------------------------

EXPLANATION FOR MPI CONTEXT:

The last argument for the functions is a pointer to an object of type
MPIContext (see header file for definition). This object contains the 
following:-
  rank      rank of this process with respect to the processes parti-
            cipating in the tridiagonal solve
  nproc     number of processes participating in the tridiagonal solve
  comm      MPI communicator
  proc      an array of size nproc containing the actual rank in comm
            for each rank 0,...,nproc

The tridiagonal solver considers the system split into nproc number of
processes with each process rank being 0,....,nproc.

If the system being solved is a part of a 1D problem, i.e, for processes 
in the communicator comm arranged as

  0 | 1 | 2 | ... | ... | nproc-1

with *all* processes participating in the solution process, then

  proc[rank] = rank, 0 <= rank < nproc
  
since the 1D arrangement of processes is identical to that assumed by the
tridiagonal solver. In this case, one can let proc be an array such that

  proc[rank] = rank

** It cannot be NULL.

However, consider a 2D problem with the grid split in both dimensions and 
processes arranged as follows:

      10 | 11 | 12 | 13 | 14
      ----------------------
    j  5 |  6 |  7 |  8 |  9
      ----------------------
       0 |  1 |  2 |  3 |  4
                i

i.e, 15 processes, with the domain split 5 parts in i and 3 parts in j. For
this example, to solve a tridiagonal system involving the middle row:

  rank  = 0,1,2,3,4
  nproc = 5
  proc  = [5,6,7,8,9]

Similarly, to solve a tridiagonal system involving the left-most column,

  rank  = 0,1,2
  nproc = 3
  proc  = [0,5,10]

This is how the MPIContext object can be used to solve tridiagonal systems
involving a subdomain and some of the processes in the communicator using
the functions provided here.

** Setting this object to NULL implies a serial run
------------------------------------------------------------------------
